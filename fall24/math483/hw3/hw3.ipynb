{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Step 1: Translation and Standardization\n\nIn this step, we center and standardize the dataset. We subtract the mean of each feature from the respective feature values (translation), and then divide by the standard deviation of each feature (standardization). This ensures that all features are on the same scale, which is important for PCA as it is sensitive to the relative scaling of features.",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\n\ndata = pd.read_csv('breast_dataset.csv')\nstdata = (data - np.mean(data, axis=0)) / np.std(data, axis=0)",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Step 2: Compute the Covariance Matrix\n\nThe covariance matrix gives us a sense of how different features of the dataset are correlated with one another. This matrix is critical for PCA because it quantifies the relationships between variables.",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "code",
      "source": "covmat = np.cov(stdata.T)",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Step 3: Eigendecomposition of the Covariance Matrix\n\nNow we perform eigendecomposition on the covariance matrix. This will give us the eigenvalues and eigenvectors, which represent the variance and direction of the principal components, respectively.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "evals, evecs = np.linalg.eig(covmat)",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Step 4: Project the Data onto the Principal Components\n\nIn this step, we project the standardized data onto the eigenvectors (principal components). This transforms the data into a new space defined by the principal components, reducing dimensionality while retaining most of the variance.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "projdata = np.dot(stdata, evecs)",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Step 5: Calculate the Percentage of Variance Explained by Each Principal Component\n\nTo evaluate the performance of PCA, we calculate how much variance each principal component explains. This helps us decide how many components to retain for optimal dimensionality reduction.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "explvar = evals / np.sum(evals)",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Step 6: Reconstruct the Data\n\nWe can reconstruct the data from the reduced set of principal components to validate our PCA transformation. This step is a useful check to ensure that we haven't lost significant information.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "recondata = np.dot(projdata, evecs.T)",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Step 7: Visualizing the Data Using the First 2 and 3 Principal Components\n\nWe can visualize the data projected onto the first 2 principal components (2D plot) and the first 3 principal components (3D plot) to see how well PCA separates the data.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8,6))\nplt.scatter(projdata[:, 0], projdata[:, 1])\nplt.title('2D PCA')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(projdata[:, 0], projdata[:, 1], projdata[:, 2])\nax.set_xlabel('Principal Component 1')\nax.set_ylabel('Principal Component 2')\nax.set_zlabel('Principal Component 3')\nplt.title('3D PCA')\nplt.show()",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Step 8: Compare Original Data and Reconstructed Data\n\nFinally, we reconstruct the data using fewer principal components (e.g., the first 2) and compare it with the original standardized data to see how much information is retained.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "reddata = projdata[:, :2]\nreconred = np.dot(reddata, evecs[:, :2].T)\n\nplt.figure(figsize=(8,6))\nplt.scatter(stdata[:, 0], stdata[:, 1], label='Original')\nplt.scatter(reconred[:, 0], reconred[:, 1], label='Reconstructed')\nplt.legend()\nplt.show()",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}