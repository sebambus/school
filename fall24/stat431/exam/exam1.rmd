---
title: "Exam 1"
author: "`r params$my_name`"
output: pdf_document
date: "2024-09-16"
params:
  my_name: "Sebastian Nuxoll"
  my_vnumber: "V00818861"
---

# Initialization

Be sure to enter your name and Vandal number in the YAML header above. Then run the chunk below to create your personalized "analysis_data" for your work. *DO NOT ALTER ANYTHING IN THIS CHUNK.*

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(as.Date("2024-09-16"))
numlist <- unlist(strsplit(params$my_vnumber, ""))
numlist <- if(length(numlist) == 9) as.numeric(numlist[2:9]) else as.numeric(numlist[1:8])
numlist <- numlist + 1
numlist <- sample(numlist) * sample(numlist) * sample(c(-1,1), length(numlist), replace = T)
set.seed(sum(abs(numlist))^2)
nobs <- max(abs(numlist[3]) %/% 10 * 10, 20) + 2
rs <- numlist[1]*runif(1,1,3)
ri <- numlist[2]+ runif(1,-5,5)
if(numlist[4] == numlist[5]) numlist[5] <- numlist[5]+5
x <- runif(nobs, min(numlist[4:5]), max(numlist[4:5]))
cxy <- rs * var(x)
r2 <- runif(1, 0, 0.5)
vy <- cxy^2/(var(x) * r2)
se.r <- sqrt(vy - rs^2*var(x))
y <- ri + rs*x + rnorm(nobs,sd=se.r)
library(tibble)
analysis_data <- tibble(independent = x, dependent = y)
rm(numlist, ri, rs, x, y, nobs,cxy,r2,se.r,vy)
```

Here is what the basic scatter plot of your data looks like:

```{r Plot data, echo=F}
plot(analysis_data$independent,analysis_data$dependent, xlab = "Independent",
     ylab = "Dependent", main = "Analysis Data")
```

# t table

Here is a table of hypothetical t-values. Use these in the construction of confidence intervals and assignment of p-values. Assume that values are ordered **smallest to largest** when going from **left to right** in the table. The table functions like the `qt()` and `pt()` commands in R. For example, `pt(-a, 10)` would give `0.025` and `pt(a,10)` would give 0.975. Similarly, `qt(0.025,10)` would give `-a`, while `qt(0.975,10)` yields `a`.

```{r t-table, echo=F}
tvals <- tibble(
  "df"=seq(10,100,by=10),
  "P = 0.025" = paste0("-",letters[1:10]),
  "P = 0.05" = paste0("-",letters[11:20]),
  "P = 0.1" = paste0("-",LETTERS[1:10]),
  "P = 0.2" = paste0("-",LETTERS[11:20]),
  "P = 0.5" = rep(0,10),
  "P = 0.8" = LETTERS[11:20],
  "P = 0.9" = LETTERS[1:10],
  "P = 0.95" = letters[11:20],
  "P = 0.975" = letters[1:10]
)

knitr::kable(tvals, caption = "Hypothetical t-values", align = "lccccccccc")
```

# Questions

1. Assume that $\bar{x}= 10$, $\bar{x^2} = 200$, $\bar{y} = 5$, $\bar{y^2} = 50$, and $\bar{xy} = 75$. What are $\beta_0, \beta_1, \sigma_{\epsilon}, \rho$ and $R^2$ for the simple regression? 
$$\beta_1=\frac{\bar{xy}-\bar{x}\bar{y}}{\bar{x^2}-\bar{x}^2}=\frac{75-10(5)}{200-10^2}=\boxed{0.25}$$
$$\beta_0=\bar{y}-\beta_1\bar{x}=5-0.25(10)=\boxed{2.5}$$
$$\rho=\frac{\bar{xy}-\bar{x}\bar{y}}{\sqrt{(\bar{x^2}-\bar{x}^2)(\bar{y^2}-\bar{y}^2)}}=\frac{75-10(5)}{\sqrt{(200-10^2)(50-5^2)}}=\boxed{0.5}$$
$$R^2=\rho^2=0.5^2=\boxed{0.25}$$
$$\sigma_\epsilon=\sqrt{(\bar{y^2}-\bar{y}^2)(1-R^2)}=\sqrt{(50-5^2)(1-0.25)}\approx\boxed{4.33}$$
 - Now assume that $x' = x + 4$ and $y' = y - 2$. Which of the parameters from the previous part changed for the simple regression of $y'$ on $x'$? What are the new values for those parameters that changed?

The only parameter affected is $B_0$, which becomes $3-0.25(14)=-0.5$

 - Lastly, assume that $x'' = 2 x$. Again, find which parameters would change and their values for the regression of $y$ on $x''$.

$B_0$, $B_1$, and $\sigma_\epsilon$ are all doubled, making them 5, 0.5, and 8.66 respectively.

2. What is the model underlying simple regression? First write the model in terms of $\mu_i = f(x_i)$. Then specify how $y_i$ relates to $\mu_i$. From that relationship, find the formula for the residual $\epsilon_i$ and give the distribution for all residuals.
$$\mu_i=\beta_0+\beta_1x_i$$
$$y_i-\mu_i=\epsilon_i\sim N(0,\sigma^2)$$
 - How many parameters are we estimating in this simple regression model?
 
 We are estimating 2 parameters: $\beta_0$ and $\beta_1$.

 - Finally, state what the assumptions are from the model you have outlined above.

We assume that there is a linear relationship between $x$ and $y$ and that error is identically distributed across the line.

3. Assume that $\hat{\beta}_1 = 2$ and $\sigma_{\hat{\beta}_1} = 4$ in a model based on 42 observations. Using the table provided above, write the 80% confidence for $\beta_1$.
$$\hat{\beta_1}\pm t_{\alpha/2}\sigma_{\hat{\beta_1}}=\boxed{2\pm4D}$$
 - Next, assume we want to test $H_0: \beta \le 5$, $H_a: \beta > 5$ at the $\alpha = 0.05$ uncertainty level. Write the inequality that we would use to decide whether or not to reject $H_0$. (In other words, if the inequality is **true** you would reject $H_0$.)
$$\frac{\hat{\beta_1}-5}{\sigma_{\hat{\beta_1}}}>t_{0.05}\Rightarrow\boxed{-0.75>n}$$
 - Finally, assume we want to test $H_0: \beta = 1$, $H_a: \beta \ne 1$ at the $\alpha = 0.1$ uncertainty level. What is the inequality that we would use to decide whether or not to reject $H_0$? 
$$\left|\frac{\hat{\beta_1}-1}{\sigma_{\hat{\beta_1}}}\right|>t_{0.05}\Rightarrow\boxed{0.25>n}$$
4. Given that $\hat{\beta}_0 = 2, \hat{\beta}_1 = 3, x = 2, n = 82, \sigma_\epsilon = 3, \text{ and } \sigma_{\hat{y}}(x = 2) = 4$. What is the 60% **confidence** interval at $x = 2$? (Use the hypothetical t table to find the correct values.)
$$\hat{\beta_0}+\hat{\beta_1}x\pm t_{\alpha/2}\sigma_{\hat{y}}(x)=2+3(2)\pm4R=\boxed{8\pm4R}$$
 - What is the 95% confidence interval for a **prediction** at $x = 2$?
$$\hat{y}\pm t_{\alpha/2}\sqrt{\sigma_{\hat{y}}^2+\sigma_\epsilon^2}=8\pm r\sqrt{4^2+9^2}=\boxed{8\pm\sqrt{97}r}$$
5. Use `lm()` to perform the simple regression of the dependent variable on the independent variable in the `analysis_data` tibble and answer the following the questions:
```{r}
model <- lm(analysis_data$dependent ~ analysis_data$independent)
sum <- summary(model)
```
 - What is the fraction of variance that is *unexplained* by the model?
 
 The fraction is `r 1-sum$r.squared`
 
 - What is the slope of the fitted line?
 
 The slope of the line is `r sum$coefficients[2,1]`
 
 - What is the probability that $\beta_0$ is 0?
 
 The probability is `r sum$coefficients[1,4]``
 
 - Assume that we are interested in whether the slope is greater than 10. Calculate the appropriate t-value. Write the R command that finds the p-value associated with that t-value and the appropriate hypothesis.
$$H_0:\hat{\beta_1}=10;H_a:\hat{\beta_1}>10$$
$$t=\frac{\hat{\beta_1}-10}{\sigma_{\hat{\beta_1}}}=`r (sum$coefficients[2,1]-10)/sum$coefficients[2,2]`$$
```{r}
pt((sum$coefficients[2,1]-10)/sum$coefficients[2,2],df=20,lower.tail=F)
```
 - Assume that we want to demonstrate that the intercept is not -5. Calculate the appropriate t-value. Write the R command that finds the p-value associated with that t-value and the appropriate hypothesis.
$$H_0:\hat{\beta_0}=-5;H_a:\hat{\beta_0}\ne -5$$
$$t=\frac{\hat{\beta_0}+5}{\sigma_{\hat{\beta_0}}}=`r (sum$coefficients[1,1]+5)/sum$coefficients[1,2]`$$
```{r}
2*pt((sum$coefficients[1,1]+5)/sum$coefficients[1,2],df=20)
```
 - Run another regression that hypothesizes that the dependent variable is related to the **squared** independent variable. Compare the two models and give an argument as to which model is the best (support your argument with values from the regressions).
```{r}
model2 <- lm(analysis_data$dependent ~ I(analysis_data$independent^2))
sum2 <- summary(model2)
```
I think the linear model is better because it has a reasonable $R^2$ of `r sum$r.squared`, while the quadratic model has an abyssmal $R^2$ of `r sum2$r.squared`.

6. Use **ggplot** to create the following the plots:
 - A plot that has the raw data and the first fitted line from (5).
```{r}
library(ggplot2)
ggplot(analysis_data, aes(x=independent, y=dependent)) +
  geom_point() +
  geom_abline(slope = sum$coefficients[2][1], intercept = sum$coefficients[1][1])
```

 - A plot that has the raw data, the first fitted line from (5), and the *confidence interval* for the line assuming that you want the interval associated with $t =\pm 1.3$.
```{r}
ggplot(analysis_data, aes(x=independent, y=dependent)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y~x, level = 0.8)
```

 - A plot that has the raw data, the first fitted line from (5), and the *prediction interval* for the line assuming that you want the interval associated with $t =\pm 2$.
```{r}
pdat <- predict(model, newdata = analysis_data, interval = "prediction", level=0.95)

ggplot(cbind(analysis_data, pdat), aes(x=independent, y=dependent)) +
  geom_point() +
  geom_abline(slope = sum$coefficients[2][1], intercept = sum$coefficients[1][1]) + 
  geom_ribbon(aes(ymin=lwr, ymax=upr), alpha = 0.3)
```

 - A plot that has the raw data and *both* of the fitted models from (5).
```{r}
library(ggplot2)
ggplot(analysis_data, aes(x=independent, y=dependent)) +
  geom_point() +
  geom_abline(slope = sum$coefficients[2][1], intercept = sum$coefficients[1][1]) + 
  geom_smooth(method = "lm", formula = y~I(x^2), se=F)
```

7. Use `dplyr` commands to do the following:
```{r}
library(dplyr)
```
 - Sort the analysis data based on the independent variable
```{r}
analysis_data <- analysis_data %>% arrange(analysis_data$independent)
```
 - Create a new column called "AB" that is a factor where 50% of values are "A" and the remainder are "B".
```{r}
analysis_data <- analysis_data %>% mutate(AB = rep(c("A", "B"), each=11))
```
 - Group the variables by "AB" and find the mean values of the independent and dependent variables using the `summarize()` command.
```{r}
grouped_data <- analysis_data %>% group_by(AB) %>%
  summarize(mean_independent = mean(independent, na.rm = TRUE),
            mean_dependent = mean(dependent, na.rm = TRUE))
grouped_data
```
 - Drop all observations from the data that are in the lower quartile of the dependent variable.
```{r}
analysis_data <- analysis_data %>% filter(dependent > quantile(dependent, 0.25))
```
 - Create a new column called "Transform" that contains a mathematical transform of the dependent and independent variables (you can choose whatever function you want).
```{r}
analysis_data <- analysis_data %>% mutate(Transform = independent^2 + dependent^2)
```
 - Create a new tibble that only retains the "AB" and the "Transform" columns.
```{r}
more_data <- analysis_data %>% select(AB, Transform)
```
 - Print out the summary of this newest tibble. 
 ```{r}
 summary(more_data)
 ```