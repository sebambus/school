---
title: "Chapter 13 Homework"
author: "Sebastian Nuxoll"
date: "`r Sys.Date()`"
output: 
  pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Homework 13

Using the ex13-17.txt data set, answer the following questions. The data set has 4 variables of interest and comes from "The Influence of Temperature and Sunshine on the Alpha-Acid Content of Hops" [Agricultural Meteorology (1974) 12:375-382]. In short, we want to model the alpha acid content (P) with respect to variety (Variety), temperature (T), and sunshine (S); for now, we will ignore the 'Field' variable that is in the data set. We wish to consider models that span:

$$P = \beta_0$$
to


\begin{align*}
P & = \beta_0 + \beta_1 T + \beta_2 T^2 + \beta_3 S + \beta_4 S^2 + \beta_5 T \times S + \beta_6 \text{Variety} + \\
  &\beta_7 \text{Variety} \times T + \beta_8 \text{Variety} \times T^2 +  \beta_9 \text{Variety} \times S +  \beta_{10} \text{Variety} \times S^2 + \\
  & \beta_{11} \text{Variety} \times T \times S
\end{align*}


1. Using forward selection based on the F-statistic, add variables sequentially to find the best model. Use a cutoff value of p = 0.1 for a new variable to enter the model.

---------------

### ANSWER:
```{r}
dat <- read.csv('ex13-17.txt', quote = "'")

form <- P ~ T + I(T^2) + S + I(S^2) + T:S + Variety + T:Variety + I(T^2):Variety + S:Variety + I(S^2):Variety + T:S:Variety

forwmodel <- lm(P~1, dat)
repeat {
  perms <- add1(forwmodel, form, test = "F")
  if(min(perms$'Pr(>F)', na.rm = T) > 0.1) break
  forwmodel <- update(forwmodel, as.formula(paste(". ~ . + ", rownames(perms)[which.min(perms$'Pr(>F)')])))
}
```
Forward selection gives us a model of $P\sim `r formula(forwmodel)[3]`$

---------------

2. Using backward selection based on the $\chi^2$ statistic, drop variables sequentially to find the best model. Use a cutoff value of p = 0.15 for a variable to remain in the model.

---------------

### ANSWER:
```{r}
backmodel <- step(lm(form, dat), direction='backward', trace = F)
backmodel <- lm(form, dat)
repeat {
  perms <- drop1(backmodel, test = "Chisq")
  if(max(perms$'Pr(>Chi)', na.rm = T) < 0.15) break
  backmodel <- update(backmodel, as.formula(paste(". ~ . - ", rownames(perms)[which.max(perms$'Pr(>Chi)')])))
}
```
Backward selection gives us a model of $P\sim `r formula(backmodel)[3]`$

---------------

3. Using stepwise selection, start from a model that includes the intercept, temperature, sunshine, and variety. Let the stepwise procedure consider everything from our base model to our full model (given above).

---------------

### ANSWER:
```{r}
stepmodel <- step(lm(P ~ T + S + Variety, dat), scope = list(upper = form, lower = P ~ 1), trace = F)
```
Stepwise selection gives us a model of $P\sim `r formula(stepmodel)[3]`$

---------------

4. Create a table that has the following information: There will be 5 rows, one row for each of the chosen models (Q1-3) and a row for the base model and a row for the full model. In the columns give: $R^2$, adjusted $R^2$, PRESS, Mallow's Cp,  AIC, and BIC (so 6 columns of stats, plus one column giving the model name).

---------------

### ANSWER:
```{r, echo=F}
library(knitr)

basemodel <- lm(P ~ 1, dat)
fullmodel <- lm(form, dat)
models <- list(basemodel, fullmodel, forwmodel, backmodel, stepmodel)
table <- data.frame()

for(model in models) {
  table = rbind(table, c(summary(model)$r.squared,
                         summary(model)$adj.r.squared,
                         sum((residuals(model) / (1 - hatvalues(model)))^2),
                         sum(residuals(model)^2) / summary(model)$sigma^2 - length(coefficients(model)),
                         AIC(model),
                         BIC(model)))
}

names(table) <- c("$R^2$", "Adj. $R^2$", "PRESS", "Mallow's Cp", "AIC", "BIC")
rownames(table) <- c("Base", "Forward", "Backward", "Stepwise", "Full")

kable(table, row.names = T, digits=4)
```

---------------

5. Calculate the F-statistic and p-value for the F-statistic to compare the 5 models, as we did in Chapter 12. (So compare them in sequentially increasing order of complexity.)

---------------

### ANSWER:
```{r}
base2back <- anova(basemodel, backmodel)
back2full <- anova(backmodel, fullmodel)
full2step <- anova(forwmodel, stepmodel)
step2forw <- anova(stepmodel, forwmodel)
```
When we compare the base model to the backward model, we get a F of `r base2back[2,"F"]` and a p of `r base2back[2,"Pr(>F)"]`.
When we compare the backward model to the full model, we get a F of `r back2full[2,"F"]` and a p of `r back2full[2,"Pr(>F)"]`.
When we compare the full model to the stepwise model, we get a F of `r full2step[2,"F"]` and a p of `r full2step[2,"Pr(>F)"]`.
When we compare the stepwise model to the forward model, we get a F of `r step2forw[2,"F"]` and a p of `r step2forw[2,"Pr(>F)"]`.

---------------
  
6. Plot the diagnostic plots of the forward selection model (Q1) and comment on any problems you see and how you might address them.

---------------

### ANSWER:
```{r, echo=F}
plot(forwmodel)
```

---------------

7. Make an argument about which model you would choose and why! INTERPRET YOUR MODEL. Are there separate equations for the different varieties? Give stats to support your argument.

---------------

### ANSWER:
The stepwise model performs well based on the metrics (Adjusted $R^2$ = `r table$'Adj. $R^2$'[4]`, AIC = `r table$AIC[4]`, BIC = `r table$BIC[4]`). It captures the important relationships without being overly complex and reflects each aspect of the model well. It includes variety, sunshine, and temperature individually, and also includes temperature times sunshine, which makes sense, since these variables are clearly correlated. Therefore, I would choose the stepwise model as it balances simplicity and accuracy, avoids overfitting, and includes meaningful interactions.

---------------

8. Calculate the VIF for each term in your model and make a table of the results. Which term shows the highest collinearity with the others?

---------------

### ANSWER:
```{r, echo=F}
suppressMessages(library(car))
suppressMessages(v <- vif(stepmodel))
suppressMessages(kable(vif(stepmodel)[,1], col.names = c("VIF")))
```
The T:S term has by far the most collinearity with the others.

---------------
